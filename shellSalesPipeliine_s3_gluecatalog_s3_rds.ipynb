{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading Files to S3\n",
    "\n",
    "1. Ensure your boto3 session is established and working\n",
    "2. Required sales_data csv file is in the source folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the libraries, if not installed then run pip3 install -r requirements.txt to install them\n",
    "import boto3\n",
    "import awswrangler as wr\n",
    "import pandas as pd\n",
    "import configparser\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import psycopg2\n",
    "from io import StringIO\n",
    "import configparser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the credentials securely.\n",
    "credentials = configparser.ConfigParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use read_file method\n",
    "credentials.read_file(open('credentials.config'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading in the credentials into Python variables. No one can see them\n",
    "aws_key = credentials[\"AWS\"][\"KEY\"]\n",
    "aws_secret = credentials[\"AWS\"][\"SECRET\"]\n",
    "region = credentials[\"AWS\"][\"REGION\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the Session\n",
    "your_session = boto3.Session(aws_access_key_id=aws_key,\n",
    "                            aws_secret_access_key=aws_secret,\n",
    "                            region_name=region)\n",
    "glue = boto3.client('glue', region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create variable with destination bucket path\n",
    "destination = 's3://shellsalespipelinedata'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "### create variable with source data path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_file='source_folder'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_datasales_data.csv  Normalized  sales_data.csv\n"
     ]
    }
   ],
   "source": [
    "#Confirming data in source_folder/sales_data.csv exists\n",
    "!ls source_folder/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s3://shellsalespipelinedata/cleaned/clean_data.csv',\n",
       " 's3://shellsalespipelinedata/sales_data.csv']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#list objects destination folder to confirm it's empty\n",
    "wr.s3.list_objects(path=destination,boto3_session=your_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using wr.s3.upload() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "wr.s3.upload(local_file +'/sales_data.csv',\n",
    "             path=destination + '/sales_data.csv',\n",
    "            boto3_session=your_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Verifying the file has uploaded successfully using list_objects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s3://shellsalespipelinedata/cleaned/clean_data.csv',\n",
       " 's3://shellsalespipelinedata/sales_data.csv']"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wr.s3.list_objects(path=destination,boto3_session=your_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loading csv file into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_csv = pd.read_csv(\"source_folder/sales_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Remove duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df_cleaned = sales_csv.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Remove rows with any null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = sales_df_cleaned.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows: 0\n"
     ]
    }
   ],
   "source": [
    "# print(df_cleaned.isnull().sum())\n",
    "duplicates = df_cleaned.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {duplicates}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Convert the DataFrame back to CSV format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_local_file_path = local_file +'/clean_datasales_data.csv'\n",
    "csv_cleaned = df_cleaned.to_csv(cleaned_local_file_path,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Upload the cleaned CSV back to S3 cleaned folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "wr.s3.upload(local_file +'/clean_datasales_data.csv',\n",
    "             path=destination + '/cleaned/clean_data.csv',\n",
    "            boto3_session=your_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization been done on the cleaned_data files uploaded in folder named normalized in s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Distributor Name</th>\n",
       "      <th>DC Number</th>\n",
       "      <th>DC Name</th>\n",
       "      <th>Sales Month</th>\n",
       "      <th>Sales Day</th>\n",
       "      <th>Sales Year</th>\n",
       "      <th>Distributor SKU</th>\n",
       "      <th>Shell SKU Number</th>\n",
       "      <th>Distributor SKU Description</th>\n",
       "      <th>DFOA Quantity</th>\n",
       "      <th>Non-DFOA Quantity</th>\n",
       "      <th>Unit of Measure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>X Petroleum</td>\n",
       "      <td>1001</td>\n",
       "      <td>Warehouse A</td>\n",
       "      <td>8</td>\n",
       "      <td>31</td>\n",
       "      <td>2024</td>\n",
       "      <td>ABC123-GL</td>\n",
       "      <td>1</td>\n",
       "      <td>Product A Description</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>GL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>X Petroleum</td>\n",
       "      <td>1001</td>\n",
       "      <td>Warehouse A</td>\n",
       "      <td>8</td>\n",
       "      <td>31</td>\n",
       "      <td>2024</td>\n",
       "      <td>DEF456-GL</td>\n",
       "      <td>2</td>\n",
       "      <td>Product B Description</td>\n",
       "      <td>188</td>\n",
       "      <td>2204</td>\n",
       "      <td>GL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>X Petroleum</td>\n",
       "      <td>1002</td>\n",
       "      <td>Warehouse B</td>\n",
       "      <td>8</td>\n",
       "      <td>31</td>\n",
       "      <td>2024</td>\n",
       "      <td>DEF456-GL</td>\n",
       "      <td>2</td>\n",
       "      <td>Product B Description</td>\n",
       "      <td>150</td>\n",
       "      <td>1709</td>\n",
       "      <td>GL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>X Petroleum</td>\n",
       "      <td>1001</td>\n",
       "      <td>Warehouse A</td>\n",
       "      <td>8</td>\n",
       "      <td>31</td>\n",
       "      <td>2024</td>\n",
       "      <td>GHI789-D55GL</td>\n",
       "      <td>3</td>\n",
       "      <td>Product C Description</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>D55GL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>X Petroleum</td>\n",
       "      <td>1002</td>\n",
       "      <td>Warehouse B</td>\n",
       "      <td>8</td>\n",
       "      <td>31</td>\n",
       "      <td>2024</td>\n",
       "      <td>JKL012-D55GL</td>\n",
       "      <td>4</td>\n",
       "      <td>Product D Description</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>D55GL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Distributor Name  DC Number      DC Name  Sales Month  Sales Day  \\\n",
       "0      X Petroleum       1001  Warehouse A            8         31   \n",
       "1      X Petroleum       1001  Warehouse A            8         31   \n",
       "2      X Petroleum       1002  Warehouse B            8         31   \n",
       "3      X Petroleum       1001  Warehouse A            8         31   \n",
       "4      X Petroleum       1002  Warehouse B            8         31   \n",
       "\n",
       "   Sales Year Distributor SKU  Shell SKU Number Distributor SKU Description  \\\n",
       "0        2024       ABC123-GL                 1       Product A Description   \n",
       "1        2024       DEF456-GL                 2       Product B Description   \n",
       "2        2024       DEF456-GL                 2       Product B Description   \n",
       "3        2024    GHI789-D55GL                 3       Product C Description   \n",
       "4        2024    JKL012-D55GL                 4       Product D Description   \n",
       "\n",
       "   DFOA Quantity  Non-DFOA Quantity Unit of Measure  \n",
       "0           1000                  0              GL  \n",
       "1            188               2204              GL  \n",
       "2            150               1709              GL  \n",
       "3              4                  1           D55GL  \n",
       "4              3                  0           D55GL  "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset (for example from S3)\n",
    "s3_bucket = 'your-bucket'\n",
    "s3_key = destination + '/clean_datasales_data'\n",
    "\n",
    "data = wr.s3.read_csv(path=s3_key, boto3_session=your_session)\n",
    "\n",
    "\n",
    "# Display the first few rows of the data\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distributor Table:\n",
      "   DistributorID Distributor Name\n",
      "0              1      X Petroleum\n",
      "\n",
      "DC Table:\n",
      "   DC Number      DC Name  DCID  DistributorID\n",
      "0       1001  Warehouse A     1              1\n",
      "1       1002  Warehouse B     2              1\n",
      "\n",
      "Sales Date Table:\n",
      "   Sales Month  Sales Day  Sales Year  DateID\n",
      "0            8         31        2024       1\n",
      "\n",
      "SKU Table:\n",
      "   Distributor SKU  Shell SKU Number Distributor SKU Description  SKU_ID\n",
      "0        ABC123-GL                 1       Product A Description       1\n",
      "1        DEF456-GL                 2       Product B Description       2\n",
      "2     GHI789-D55GL                 3       Product C Description       3\n",
      "3     JKL012-D55GL                 4       Product D Description       4\n",
      "4     MNO345-D55GL                 5       Product E Description       5\n",
      "5        PQR678-GL                 6       Product F Description       6\n",
      "6     STU901-D209L                 7       Product G Description       7\n",
      "7     VWX234-D55GL                 8       Product H Description       8\n",
      "8     YZA567-K50KG                 9       Product I Description       9\n",
      "9     BCD890-PL20L                10       Product J Description      10\n",
      "10    EFG123-D209L                11       Product K Description      11\n",
      "11    HIJ456-D55GL                12       Product L Description      12\n",
      "12    KLM789-C6/1Q                13       Product M Description      13\n",
      "13    NOP012-C6/1Q                14       Product N Description      14\n",
      "14    QRS345-D55GL                15       Product O Description      15\n",
      "15    TUV678-PL5GL                16       Product P Description      16\n",
      "16    WXY901-PL5GL                17       Product Q Description      17\n",
      "17    ZAB234-C6/1Q                18       Product R Description      18\n",
      "18    CDE567-C6/1Q                19       Product S Description      19\n",
      "19    FGH890-D55GL                20       Product T Description      20\n",
      "20    IJK012-D55GL                21       Product U Description      21\n",
      "21    LMN345-D55GL                22       Product V Description      22\n",
      "22    OPQ678-D55GL                23       Product W Description      23\n",
      "23    RST901-D55GL                24       Product X Description      24\n",
      "24    UVW234-D55GL                25       Product Y Description      25\n",
      "25    XYZ567-D55GL                26       Product Z Description      26\n",
      "26    ABC123-D55GL                27      Product AA Description      27\n",
      "27    DEF456-D55GL                28      Product BB Description      28\n",
      "\n",
      "Sales Table:\n",
      "    DC Number Distributor SKU  DFOA Quantity  Non-DFOA Quantity  \\\n",
      "0        1001       ABC123-GL           1000                  0   \n",
      "1        1001       DEF456-GL            188               2204   \n",
      "2        1002       DEF456-GL            150               1709   \n",
      "3        1001    GHI789-D55GL              4                  1   \n",
      "4        1002    JKL012-D55GL              3                  0   \n",
      "5        1001    MNO345-D55GL              2                  0   \n",
      "6        1001       PQR678-GL              0                  0   \n",
      "7        1001    STU901-D209L              0                  0   \n",
      "8        1002    VWX234-D55GL              0                  1   \n",
      "9        1002    YZA567-K50KG              0                  4   \n",
      "10       1001    BCD890-PL20L              0                  0   \n",
      "11       1002    EFG123-D209L              0                  1   \n",
      "12       1001    HIJ456-D55GL              0                  0   \n",
      "13       1002    KLM789-C6/1Q              0                  0   \n",
      "14       1002    NOP012-C6/1Q              0                  0   \n",
      "15       1002    QRS345-D55GL              0                  8   \n",
      "16       1002    TUV678-PL5GL              0                  0   \n",
      "17       1002    WXY901-PL5GL              0                  0   \n",
      "18       1002    ZAB234-C6/1Q              0                  0   \n",
      "19       1002    CDE567-C6/1Q              0                  0   \n",
      "20       1002    FGH890-D55GL              0                  1   \n",
      "21       1001    IJK012-D55GL              0                  2   \n",
      "22       1001    LMN345-D55GL              0                  2   \n",
      "23       1001    OPQ678-D55GL              0                  1   \n",
      "24       1001    RST901-D55GL              0                  1   \n",
      "25       1002    UVW234-D55GL              0                  1   \n",
      "26       1002    XYZ567-D55GL              0                  0   \n",
      "27       1001    ABC123-D55GL              0                  0   \n",
      "28       1002    DEF456-D55GL              0                  0   \n",
      "29       1001       ABC123-GL           1000                  0   \n",
      "30       1001       DEF456-GL            188               2204   \n",
      "31       1002       DEF456-GL            150               1709   \n",
      "32       1002    JKL012-D55GL              3                  0   \n",
      "33       1001    MNO345-D55GL              2                  0   \n",
      "34       1001       PQR678-GL              0                  0   \n",
      "35       1001    STU901-D209L              0                  0   \n",
      "36       1002    VWX234-D55GL              0                  1   \n",
      "37       1002    YZA567-K50KG              0                  4   \n",
      "38       1001    BCD890-PL20L              0                  0   \n",
      "39       1002    EFG123-D209L              0                  1   \n",
      "40       1002    KLM789-C6/1Q              0                  0   \n",
      "41       1002    NOP012-C6/1Q              0                  0   \n",
      "42       1002    TUV678-PL5GL              0                  0   \n",
      "43       1002    WXY901-PL5GL              0                  0   \n",
      "44       1002    ZAB234-C6/1Q              0                  0   \n",
      "45       1002    CDE567-C6/1Q              0                  0   \n",
      "46       1002    FGH890-D55GL              0                  1   \n",
      "\n",
      "   Unit of Measure  SaleID  DCID  SKU_ID  DateID  \n",
      "0               GL       1     1       1       1  \n",
      "1               GL       2     1       2       1  \n",
      "2               GL       3     2       2       1  \n",
      "3            D55GL       4     1       3       1  \n",
      "4            D55GL       5     2       4       1  \n",
      "5            D55GL       6     1       5       1  \n",
      "6               GL       7     1       6       1  \n",
      "7            D209L       8     1       7       1  \n",
      "8            D55GL       9     2       8       1  \n",
      "9            K50KG      10     2       9       1  \n",
      "10           PL20L      11     1      10       1  \n",
      "11           D209L      12     2      11       1  \n",
      "12           D55GL      13     1      12       1  \n",
      "13           C6/1Q      14     2      13       1  \n",
      "14           C6/1Q      15     2      14       1  \n",
      "15           D55GL      16     2      15       1  \n",
      "16           12/.4      17     2      16       1  \n",
      "17           12/.4      18     2      17       1  \n",
      "18           10/.4      19     2      18       1  \n",
      "19           PL5GL      20     2      19       1  \n",
      "20           PL5GL      21     2      20       1  \n",
      "21           C3/1G      22     1      21       1  \n",
      "22           C6/1Q      23     1      22       1  \n",
      "23           C6/1Q      24     1      23       1  \n",
      "24           PL5GL      25     1      24       1  \n",
      "25           PL5GL      26     2      25       1  \n",
      "26           C6/1Q      27     2      26       1  \n",
      "27           D55GL      28     1      27       1  \n",
      "28           D55GL      29     2      28       1  \n",
      "29           T975L      30     1       1       1  \n",
      "30           C3/1G      31     1       2       1  \n",
      "31           C6/1Q      32     2       2       1  \n",
      "32           C6/1Q      33     2       4       1  \n",
      "33           PL5GL      34     1       5       1  \n",
      "34           PL5GL      35     1       6       1  \n",
      "35              GL      36     1       7       1  \n",
      "36              GL      37     2       8       1  \n",
      "37           C6/1G      38     2       9       1  \n",
      "38           C6/1G      39     1      10       1  \n",
      "39           PL20L      40     2      11       1  \n",
      "40           D55GL      41     2      13       1  \n",
      "41           D55GL      42     2      14       1  \n",
      "42           D55GL      43     2      16       1  \n",
      "43           D55GL      44     2      17       1  \n",
      "44           D209L      45     2      18       1  \n",
      "45           C6/1Q      46     2      19       1  \n",
      "46           C6/1Q      47     2      20       1  \n"
     ]
    }
   ],
   "source": [
    "# Create Distributor Table\n",
    "distributor_df = pd.DataFrame({\n",
    "    'DistributorID': [1],\n",
    "    'Distributor Name': ['X Petroleum']\n",
    "})\n",
    "\n",
    "# Create DC Table\n",
    "dc_df = data[['DC Number', 'DC Name']].drop_duplicates().reset_index(drop=True)\n",
    "dc_df['DCID'] = dc_df.index + 1\n",
    "dc_df['DistributorID'] = 1\n",
    "\n",
    "# Create Sales Date Table\n",
    "date_df = data[['Sales Month', 'Sales Day', 'Sales Year']].drop_duplicates().reset_index(drop=True)\n",
    "date_df['DateID'] = date_df.index + 1\n",
    "\n",
    "# Create SKU Table\n",
    "sku_df = data[['Distributor SKU', 'Shell SKU Number', 'Distributor SKU Description']].drop_duplicates().reset_index(drop=True)\n",
    "sku_df['SKU_ID'] = sku_df.index + 1\n",
    "\n",
    "# Create Sales Table\n",
    "sales_df = data[['DC Number', 'Distributor SKU', 'DFOA Quantity', 'Non-DFOA Quantity', 'Unit of Measure']]\n",
    "sales_df['SaleID'] = sales_df.index + 1\n",
    "\n",
    "# Merge to create foreign key relationships\n",
    "sales_df = sales_df.merge(dc_df[['DC Number', 'DCID']], on='DC Number', how='left')\n",
    "sales_df = sales_df.merge(sku_df[['Distributor SKU', 'SKU_ID']], on='Distributor SKU', how='left')\n",
    "sales_df['DateID'] = 1  # Assuming all sales happen on the same date\n",
    "\n",
    "# Final DataFrames\n",
    "print(\"Distributor Table:\")\n",
    "print(distributor_df)\n",
    "print(\"\\nDC Table:\")\n",
    "print(dc_df)\n",
    "print(\"\\nSales Date Table:\")\n",
    "print(date_df)\n",
    "print(\"\\nSKU Table:\")\n",
    "print(sku_df)\n",
    "print(\"\\nSales Table:\")\n",
    "print(sales_df)\n",
    "# wr.config.init_session(your_session)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrames uploaded successfully to S3!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Temporary local CSV files\n",
    "local_distributor_path = '/tmp/distributor_table.csv'\n",
    "local_dc_path = '/tmp/dc_table.csv'\n",
    "local_date_path = '/tmp/sales_date_table.csv'\n",
    "local_sku_path = '/tmp/sku_table.csv'\n",
    "local_sales_path = '/tmp/sales_table.csv'\n",
    "\n",
    "# Save DataFrames to local CSV files\n",
    "distributor_df.to_csv(local_distributor_path, index=False)\n",
    "dc_df.to_csv(local_dc_path, index=False)\n",
    "date_df.to_csv(local_date_path, index=False)\n",
    "sku_df.to_csv(local_sku_path, index=False)\n",
    "sales_df.to_csv(local_sales_path, index=False)\n",
    "\n",
    "# Upload local CSV files to S3\n",
    "wr.s3.upload(local_distributor_path, path=destination + '/uploadedvianotebooks/distributor_table.csv', boto3_session=your_session)\n",
    "wr.s3.upload(local_distributor_path, path=destination + '/uploadedvianotebooks/dc_table.csv', boto3_session=your_session)\n",
    "wr.s3.upload(local_distributor_path, path=destination + '/uploadedvianotebooks/sales_date_table.csv', boto3_session=your_session)\n",
    "wr.s3.upload(local_distributor_path, path=destination + '/uploadedvianotebooks/sku_table.csv', boto3_session=your_session)\n",
    "wr.s3.upload(local_distributor_path, path=destination + '/uploadedvianotebooks/sales_table.csv', boto3_session=your_session)\n",
    "\n",
    "\n",
    "# # You may clean up local files if needed\n",
    "# os.remove(local_distributor_path)\n",
    "# os.remove(local_dc_path)\n",
    "# os.remove(local_date_path)\n",
    "# os.remove(local_sku_path)\n",
    "# os.remove(local_sales_path)\n",
    "\n",
    "print(\"DataFrames uploaded successfully to S3!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data from s3 to RDS Postgresql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "### schema for the rds tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  1. Create Distributor Table\n",
    "# CREATE TABLE Distributor (\n",
    "#     DistributorID SERIAL PRIMARY KEY,\n",
    "#     DistributorName VARCHAR(255) NOT NULL\n",
    "# );\n",
    "\n",
    "#  2. Create DC (Distribution_Center) Table\n",
    "# CREATE TABLE DC (\n",
    "#     DCID SERIAL PRIMARY KEY,\n",
    "#     DCNumber VARCHAR(10) NOT NULL,\n",
    "#     DCName VARCHAR(255) NOT NULL,\n",
    "#     DistributorID INT REFERENCES Distributor(DistributorID)\n",
    "# );\n",
    "\n",
    "#  3. Create Sales_Date Table\n",
    "# CREATE TABLE SalesDate (\n",
    "#     DateID SERIAL PRIMARY KEY,\n",
    "#     SalesMonth INT NOT NULL,\n",
    "#     SalesDay INT NOT NULL,\n",
    "#     SalesYear INT NOT NULL\n",
    "# );\n",
    "\n",
    "#  4. Create SKU Table\n",
    "# CREATE TABLE SKU (\n",
    "#     SKU_ID SERIAL PRIMARY KEY,\n",
    "#     DistributorSKU VARCHAR(50) NOT NULL,\n",
    "#     ShellSKUNumber INT NOT NULL,\n",
    "#     DistributorSKUDescription VARCHAR(255) NOT NULL\n",
    "# );\n",
    "\n",
    "#  5. Create Sales Table\n",
    "# CREATE TABLE Sales (\n",
    "#     SaleID SERIAL PRIMARY KEY,\n",
    "#     DCID INT REFERENCES DC(DCID),\n",
    "#     DateID INT REFERENCES SalesDate(DateID),\n",
    "#     SKU_ID INT REFERENCES SKU(SKU_ID),\n",
    "#     DFOAQuantity INT NOT NULL,\n",
    "#     NonDFOAQuantity INT NOT NULL,\n",
    "#     UnitOfMeasure VARCHAR(50) NOT NULL\n",
    "# );\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import awswrangler as wr\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import psycopg2\n",
    "from psycopg2 import pool\n",
    "\n",
    "# Example of setting up a connection pool\n",
    "connection_pool = pool.SimpleConnectionPool(1, 10, \n",
    "    user=rds_username,\n",
    "    password=rds_password,\n",
    "    host=rds_host,\n",
    "    port=rds_port,\n",
    "    database=rds_dbname\n",
    ")\n",
    "\n",
    "def load_csv_to_postgres(table_name, boto3_session):\n",
    "    # Load CSV data from S3 using awswrangler\n",
    "    s3_key = f'{s3_prefix}{table_name}.csv'  # Construct the S3 key for the CSV file\n",
    "    \n",
    "    # Read the CSV into a DataFrame\n",
    "    df = wr.s3.read_csv(path=f's3://{s3_bucket}/{s3_key}', boto3_session=boto3_session)\n",
    "\n",
    "    # Get a connection from the pool\n",
    "    connection = connection_pool.getconn()\n",
    "    try:\n",
    "        cursor = connection.cursor()\n",
    "        \n",
    "        # Create a StringIO object to hold CSV data for bulk insertion\n",
    "        csv_buffer = StringIO()\n",
    "        df.to_csv(csv_buffer, index=False, header=False)  # Write DataFrame to StringIO as CSV without header\n",
    "        csv_buffer.seek(0)  # Move the cursor to the beginning of the StringIO buffer\n",
    "\n",
    "        # Use the COPY command for efficient bulk insertion into PostgreSQL\n",
    "        cursor.copy_from(csv_buffer, table_name, sep=',')  # Insert data from the buffer into the specified table\n",
    "        \n",
    "        connection.commit()  # Commit the transaction after successful insertion\n",
    "\n",
    "    except Exception as e:\n",
    "        connection.rollback()  # Rollback the transaction in case of error\n",
    "        print(f\"Error loading data into {table_name}: {e}\")  # Print the error message\n",
    "\n",
    "    finally:\n",
    "        cursor.close()  # Ensure cursor is closed\n",
    "        connection_pool.putconn(connection)  # Return the connection to the pool\n",
    "\n",
    "# Load each table from the list into PostgreSQL\n",
    "tables = ['DC', 'sales_table', 'Distributor', 'SalesDate','sku', 'sales']\n",
    "for table in tables:\n",
    "    load_csv_to_postgres(table, your_session)  # Call the function for each table\n",
    "\n",
    "# No need to close cursor or connection here, as it's handled in the function\n",
    "\n",
    "print(\"Data loaded successfully into PostgreSQL.\")  # Confirmation message\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Query on RDS Postgresql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to the database\n",
      "   distributorid distributorname\n",
      "0              1     X Petroleum\n",
      "Connection closed\n"
     ]
    }
   ],
   "source": [
    "import configparser\n",
    "import psycopg2  # Use pymysql for MySQL\n",
    "import pandas as pd\n",
    "\n",
    "# Read the configuration file\n",
    "config = configparser.ConfigParser()\n",
    "config.read('credent.config')\n",
    "\n",
    "# Database using env variables connection parameters\n",
    "db_params = {\n",
    "    'host': config['database']['host'],\n",
    "    'database': config['database']['dbname'],\n",
    "    'user': config['database']['username'],\n",
    "    'password': config['database']['password'],\n",
    "    'port': config['database'].getint('port')  # Convert to int\n",
    "}\n",
    "\n",
    "# Connect to the database\n",
    "try:\n",
    "    connection = psycopg2.connect(**db_params)\n",
    "    cursor = connection.cursor()\n",
    "    print(\"Connected to the database\")\n",
    "\n",
    "    # Example query\n",
    "    query = \"SELECT * FROM Distributor;\"\n",
    "    cursor.execute(query)\n",
    "    \n",
    "    # Fetch the data into a pandas DataFrame\n",
    "    data = cursor.fetchall()\n",
    "    column_names = [desc[0] for desc in cursor.description]\n",
    "    df = pd.DataFrame(data, columns=column_names)\n",
    "\n",
    "    # Display the DataFrame\n",
    "    print(df)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "finally:\n",
    "    if cursor:\n",
    "        cursor.close()\n",
    "    if connection:\n",
    "        connection.close()\n",
    "        print(\"Connection closed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create indexes and Materialized views for high performance queries before executing query tasks by Odete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import configparser\n",
    "\n",
    "# Read database credentials from config file\n",
    "config = configparser.ConfigParser()\n",
    "config.read('credent.config')\n",
    "\n",
    "# Extract credentials\n",
    "db_host = config['database']['host']\n",
    "db_name = config['database']['dbname']\n",
    "db_user = config['database']['username']\n",
    "db_password = config['database']['password']\n",
    "\n",
    "# Initialize connection and cursor\n",
    "conn = None\n",
    "cursor = None\n",
    "\n",
    "try:\n",
    "    # Establish the connection\n",
    "    conn = psycopg2.connect(\n",
    "        host=db_host,\n",
    "        database=db_name,\n",
    "        user=db_user,\n",
    "        password=db_password\n",
    "    )\n",
    "\n",
    "    # Create a cursor object\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # SQL commands\n",
    "    sql_commands = \"\"\"\n",
    "    -- Create Indexes\n",
    "    CREATE INDEX IF NOT EXISTS idx_dc_number ON DC(DCNumber);\n",
    "    CREATE INDEX IF NOT EXISTS idx_distributor_name ON Distributor(DistributorName);\n",
    "    CREATE INDEX IF NOT EXISTS idx_sales_date ON SalesDate(SalesMonth, SalesDay, SalesYear);\n",
    "    CREATE INDEX IF NOT EXISTS idx_sku ON SKU(DistributorSKU);\n",
    "    CREATE INDEX IF NOT EXISTS idx_dc_id ON Sales(DCID);\n",
    "    CREATE INDEX IF NOT EXISTS idx_date_id ON Sales(DateID);\n",
    "    CREATE INDEX IF NOT EXISTS idx_sku_id ON Sales(SKU_ID);\n",
    "\n",
    "    -- Create Materialized View\n",
    "    CREATE MATERIALIZED VIEW IF NOT EXISTS mv_sales_summary AS\n",
    "    SELECT \n",
    "        d.DistributorName,\n",
    "        dc.DCName,\n",
    "        sd.SalesMonth,\n",
    "        sd.SalesYear,\n",
    "        s.SKU_ID,\n",
    "        s.DFOAQuantity,\n",
    "        s.NonDFOAQuantity,\n",
    "        s.UnitOfMeasure\n",
    "    FROM \n",
    "        Sales s\n",
    "    JOIN \n",
    "        DC dc ON s.DCID = dc.DCID\n",
    "    JOIN \n",
    "        Distributor d ON dc.DistributorID = d.DistributorID\n",
    "    JOIN \n",
    "        SalesDate sd ON s.DateID = sd.DateID;\n",
    "    \"\"\"\n",
    "\n",
    "    # Execute the SQL commands\n",
    "    cursor.execute(sql_commands)\n",
    "\n",
    "    # Commit the changes\n",
    "    conn.commit()\n",
    "\n",
    "except psycopg2.DatabaseError as e:\n",
    "    print(f\"Database error occurred: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "finally:\n",
    "    # Close the cursor and connection if they were opened\n",
    "    if cursor:\n",
    "        cursor.close()\n",
    "    if conn:\n",
    "        conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executing client requirements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. Calculate total sales revenue per product.We are missing vital data i.e unit price(priceperunit)\n",
    "## TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database error occurred: column sku.priceperunit does not exist\n",
      "LINE 7: ...FOAQuantity) + SUM(s.NonDFOAQuantity)) * COALESCE(sku.PriceP...\n",
      "                                                             ^\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Establish the connection\n",
    "    conn = psycopg2.connect(\n",
    "        host=db_host,\n",
    "        database=db_name,\n",
    "        user=db_user,\n",
    "        password=db_password\n",
    "    )\n",
    "    \n",
    "    # Create a cursor object\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # SQL query to calculate total sales revenue per product\n",
    "    revenue_query = \"\"\"\n",
    "    SELECT \n",
    "        sku.DistributorSKU,\n",
    "        sku.DistributorSKUDescription,\n",
    "        SUM(s.DFOAQuantity) AS Total_DFOA_Quantity,\n",
    "        SUM(s.NonDFOAQuantity) AS Total_Non_DFOA_Quantity,\n",
    "        (SUM(s.DFOAQuantity) + SUM(s.NonDFOAQuantity)) * COALESCE(sku.PricePerUnit, 0) AS Total_Revenue\n",
    "    FROM \n",
    "        Sales s\n",
    "    JOIN \n",
    "        SKU sku ON s.SKU_ID = sku.SKU_ID\n",
    "    GROUP BY \n",
    "        sku.DistributorSKU, sku.DistributorSKUDescription\n",
    "    ORDER BY \n",
    "        Total_Revenue DESC;\n",
    "    \"\"\"\n",
    "\n",
    "    # Execute the query\n",
    "    cursor.execute(revenue_query)\n",
    "    \n",
    "    # Fetch results\n",
    "    results = cursor.fetchall()\n",
    "\n",
    "    # Print results\n",
    "    for row in results:\n",
    "        print(f\"SKU: {row[0]}, Description: {row[1]}, Total DFOA Quantity: {row[2]}, Total Non-DFOA Quantity: {row[3]}, Total Revenue: ${row[4]:.2f}\")\n",
    "\n",
    "except psycopg2.DatabaseError as e:\n",
    "    print(f\"Database error occurred: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "finally:\n",
    "    # Close the cursor and connection if they were opened\n",
    "    if cursor:\n",
    "        cursor.close()\n",
    "    if conn:\n",
    "        conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.Determine the total quantity sold per customer. Using Materialized View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Establish the connection\n",
    "    conn = psycopg2.connect(\n",
    "        host=db_host,\n",
    "        database=db_name,\n",
    "        user=db_user,\n",
    "        password=db_password\n",
    "    )\n",
    "    \n",
    "    # Create a cursor object\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # SQL command to create the materialized view\n",
    "    create_view_query = \"\"\"\n",
    "    CREATE MATERIALIZED VIEW IF NOT EXISTS mv_total_quantity_per_customer AS\n",
    "    SELECT \n",
    "        d.DistributorName AS CustomerName,\n",
    "        SUM(s.DFOAQuantity + s.NonDFOAQuantity) AS TotalQuantitySold\n",
    "    FROM \n",
    "        Sales s\n",
    "    JOIN \n",
    "        DC dc ON s.DCID = dc.DCID\n",
    "    JOIN \n",
    "        Distributor d ON dc.DistributorID = d.DistributorID\n",
    "    GROUP BY \n",
    "        d.DistributorName;\n",
    "    \"\"\"\n",
    "\n",
    "    # Execute the command to create the materialized view\n",
    "    cursor.execute(create_view_query)\n",
    "    \n",
    "    # Commit the changes\n",
    "    conn.commit()\n",
    "\n",
    "    # SQL command to refresh the materialized view\n",
    "    refresh_view_query = \"REFRESH MATERIALIZED VIEW mv_total_quantity_per_customer;\"\n",
    "\n",
    "    # Execute the command to refresh the materialized view\n",
    "    cursor.execute(refresh_view_query)\n",
    "    \n",
    "    # Commit the changes\n",
    "    conn.commit()\n",
    "\n",
    "except psycopg2.DatabaseError as e:\n",
    "    print(f\"Database error occurred: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "finally:\n",
    "    # Close the cursor and connection if they were opened\n",
    "    if cursor:\n",
    "        cursor.close()\n",
    "    if conn:\n",
    "        conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer: X Petroleum, Total Quantity Sold: 5283\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Establish the connection\n",
    "    conn = psycopg2.connect(\n",
    "        host=db_host,\n",
    "        database=db_name,\n",
    "        user=db_user,\n",
    "        password=db_password\n",
    "    )\n",
    "    \n",
    "    # Create a cursor object\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # SQL query to fetch total quantity sold per customer\n",
    "    fetch_query = \"SELECT * FROM mv_total_quantity_per_customer;\"\n",
    "\n",
    "    # Execute the query\n",
    "    cursor.execute(fetch_query)\n",
    "    \n",
    "    # Fetch results\n",
    "    results = cursor.fetchall()\n",
    "\n",
    "    # Print results\n",
    "    for row in results:\n",
    "        print(f\"Customer: {row[0]}, Total Quantity Sold: {row[1]}\")\n",
    "\n",
    "except psycopg2.DatabaseError as e:\n",
    "    print(f\"Database error occurred: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "finally:\n",
    "    # Close the cursor and connection if they were opened\n",
    "    if cursor:\n",
    "        cursor.close()\n",
    "    if conn:\n",
    "        conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3. Identify the top 5 products by revenue. Missing vital field priceperunit\n",
    "##TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database error occurred: column sku.priceperunit does not exist\n",
      "LINE 6:         SUM(s.DFOAQuantity * COALESCE(sku.PricePerUnit, 0)) ...\n",
      "                                              ^\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Establish the connection\n",
    "    conn = psycopg2.connect(\n",
    "        host=db_host,\n",
    "        database=db_name,\n",
    "        user=db_user,\n",
    "        password=db_password\n",
    "    )\n",
    "    \n",
    "    # Create a cursor object\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Step 1: Create the materialized view\n",
    "    create_mv_query = \"\"\"\n",
    "    CREATE MATERIALIZED VIEW IF NOT EXISTS mv_product_revenue AS\n",
    "    SELECT \n",
    "        sku.DistributorSKU,\n",
    "        sku.DistributorSKUDescription,\n",
    "        SUM(s.DFOAQuantity * COALESCE(sku.PricePerUnit, 0)) AS Total_DFOA_Revenue,\n",
    "        SUM(s.NonDFOAQuantity * COALESCE(sku.PricePerUnit, 0)) AS Total_Non_DFOA_Revenue,\n",
    "        SUM((s.DFOAQuantity + s.NonDFOAQuantity) * COALESCE(sku.PricePerUnit, 0)) AS Total_Revenue\n",
    "    FROM \n",
    "        Sales s\n",
    "    JOIN \n",
    "        SKU sku ON s.SKU_ID = sku.SKU_ID\n",
    "    GROUP BY \n",
    "        sku.DistributorSKU, sku.DistributorSKUDescription;\n",
    "    \"\"\"\n",
    "    \n",
    "    # Execute the creation of the materialized view\n",
    "    cursor.execute(create_mv_query)\n",
    "    conn.commit()\n",
    "\n",
    "    # Step 2: Query the materialized view to get the top 5 products by revenue\n",
    "    top_products_query = \"\"\"\n",
    "    SELECT \n",
    "        DistributorSKU,\n",
    "        DistributorSKUDescription,\n",
    "        Total_DFOA_Revenue,\n",
    "        Total_Non_DFOA_Revenue,\n",
    "        Total_Revenue\n",
    "    FROM \n",
    "        mv_product_revenue\n",
    "    ORDER BY \n",
    "        Total_Revenue DESC\n",
    "    LIMIT 5;\n",
    "    \"\"\"\n",
    "\n",
    "    # Execute the query\n",
    "    cursor.execute(top_products_query)\n",
    "    \n",
    "    # Fetch results\n",
    "    results = cursor.fetchall()\n",
    "\n",
    "    # Print results\n",
    "    for row in results:\n",
    "        print(f\"SKU: {row[0]}, Description: {row[1]}, Total DFOA Revenue: ${row[2]:.2f}, Total Non-DFOA Revenue: ${row[3]:.2f}, Total Revenue: ${row[4]:.2f}\")\n",
    "\n",
    "except psycopg2.DatabaseError as e:\n",
    "    print(f\"Database error occurred: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "finally:\n",
    "    # Close the cursor and connection if they were opened\n",
    "    if cursor:\n",
    "        cursor.close()\n",
    "    if conn:\n",
    "        conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate a monthly sales report showing total revenue and quantity sold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database error occurred: column sku.priceperunit does not exist\n",
      "LINE 7: ...M((s.DFOAQuantity + s.NonDFOAQuantity) * COALESCE(sku.PriceP...\n",
      "                                                             ^\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Establish the connection\n",
    "    conn = psycopg2.connect(\n",
    "        host=db_host,\n",
    "        database=db_name,\n",
    "        user=db_user,\n",
    "        password=db_password\n",
    "    )\n",
    "    \n",
    "    # Create a cursor object\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # SQL command to create the monthly sales report view\n",
    "    create_report_view_query = \"\"\"\n",
    "    CREATE MATERIALIZED VIEW IF NOT EXISTS mv_monthly_sales_report AS\n",
    "    SELECT \n",
    "        sd.SalesYear,\n",
    "        sd.SalesMonth,\n",
    "        SUM(s.DFOAQuantity + s.NonDFOAQuantity) AS TotalQuantitySold,\n",
    "        SUM((s.DFOAQuantity + s.NonDFOAQuantity) * COALESCE(sku.PricePerUnit, 0)) AS TotalRevenue\n",
    "    FROM \n",
    "        Sales s\n",
    "    JOIN \n",
    "        SKU sku ON s.SKU_ID = sku.SKU_ID\n",
    "    JOIN \n",
    "        SalesDate sd ON s.DateID = sd.DateID\n",
    "    GROUP BY \n",
    "        sd.SalesYear, sd.SalesMonth\n",
    "    ORDER BY \n",
    "        sd.SalesYear, sd.SalesMonth;\n",
    "    \"\"\"\n",
    "\n",
    "    # Execute the command to create the monthly sales report view\n",
    "    cursor.execute(create_report_view_query)\n",
    "    \n",
    "    # Commit the changes\n",
    "    conn.commit()\n",
    "\n",
    "    # SQL command to refresh the materialized view\n",
    "    refresh_view_query = \"REFRESH MATERIALIZED VIEW mv_monthly_sales_report;\"\n",
    "\n",
    "    # Execute the command to refresh the materialized view\n",
    "    cursor.execute(refresh_view_query)\n",
    "    \n",
    "    # Commit the changes\n",
    "    conn.commit()\n",
    "\n",
    "except psycopg2.DatabaseError as e:\n",
    "    print(f\"Database error occurred: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "finally:\n",
    "    # Close the cursor and connection if they were opened\n",
    "    if cursor:\n",
    "        cursor.close()\n",
    "    if conn:\n",
    "        conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database error occurred: relation \"mv_monthly_sales_report\" does not exist\n",
      "LINE 1: SELECT * FROM mv_monthly_sales_report;\n",
      "                      ^\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Establish the connection\n",
    "    conn = psycopg2.connect(\n",
    "        host=db_host,\n",
    "        database=db_name,\n",
    "        user=db_user,\n",
    "        password=db_password\n",
    "    )\n",
    "    \n",
    "    # Create a cursor object\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # SQL query to fetch the monthly sales report\n",
    "    fetch_report_query = \"SELECT * FROM mv_monthly_sales_report;\"\n",
    "\n",
    "    # Execute the query\n",
    "    cursor.execute(fetch_report_query)\n",
    "    \n",
    "    # Fetch results\n",
    "    results = cursor.fetchall()\n",
    "\n",
    "    # Print results\n",
    "    for row in results:\n",
    "        year, month, total_quantity, total_revenue = row\n",
    "        print(f\"Year: {year}, Month: {month}, Total Quantity Sold: {total_quantity}, Total Revenue: ${total_revenue:.2f}\")\n",
    "\n",
    "except psycopg2.DatabaseError as e:\n",
    "    print(f\"Database error occurred: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "finally:\n",
    "    # Close the cursor and connection if they were opened\n",
    "    if cursor:\n",
    "        cursor.close()\n",
    "    if conn:\n",
    "        conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
